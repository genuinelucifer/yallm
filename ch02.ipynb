{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7de89168-baa3-4154-875f-0c23a98ca2fb",
   "metadata": {},
   "source": [
    "# read the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745841b7-9e39-4d05-950b-26748fde97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('the-verdict.txt', 'r') as f:\n",
    "    book_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2d2bde-5faf-4af2-b96e-6fa1f8dad72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length = 20479\n",
      "Sample = 'I HAD always thought Jack Gisb'\n"
     ]
    }
   ],
   "source": [
    "print(\"Total length = {}\".format(len(book_text)))\n",
    "print(\"Sample = '{}'\".format(book_text[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c25faf-ef24-4a68-960d-deb02eafc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"Hello, (world). Is_ th;is-- a te:st?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39963a92-95aa-4b05-b39f-0ce72da3eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = re.split(r'([,.\\?\"_:;\\(\\)\\']|\\-\\-|\\s)', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d35a84-16a2-4e7e-bdae-3718c1198598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '(', 'world', ')', '.', 'Is', '_', 'th', ';', 'is', '--', 'a', 'te', ':', 'st', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = [tk for tk in parts if tk.strip()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6aa3a23-9b09-41f0-8869-026b13f7c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = re.split(r'([,.\\?\"_:;\\(\\)\\'!]|\\-\\-|\\s)', book_text)\n",
    "preprocessed = [tk for tk in parts if tk.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85cec404-8743-4bdf-84ce-8b6c5b05a3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens = 4690\n",
      "Sample = '['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']'\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens = {}\".format(len(preprocessed)))\n",
    "print(\"Sample = '{}'\".format(preprocessed[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e60ac44-e13a-460e-a7dc-55d162be466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tks = sorted(set(preprocessed))\n",
    "vocab = {tk: idx for idx, tk in enumerate(unique_tks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b4cd71e-4b69-4d0d-8dbc-9e1d67850394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9), ('?', 10), ('A', 11), ('Ah', 12), ('Among', 13), ('And', 14), ('Are', 15), ('Arrt', 16), ('As', 17), ('At', 18), ('Be', 19)]\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(list(vocab.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b864672-c940-4a91-92ce-8b2a80fec573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, passage):\n",
    "        parts = re.split(r'([,.\\?\"_:;\\(\\)\\'!]|\\-\\-|\\s)', passage)\n",
    "        preprocessed = [tk for tk in parts if tk.strip()]\n",
    "        return [self.str_to_int[tk] for tk in preprocessed]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        processed = \" \".join(self.int_to_str[tk] for tk in tokens)\n",
    "        r1 = re.sub(r'\\s+([,.:;_\\)\"\\']|--)', r'\\1', processed)\n",
    "        # for ( & -- we should remove the suffix spaces\n",
    "        return re.sub(r'([\\(]|--)\\s+', r'\\1', r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcfbe03f-d90a-4255-a610-88f3a562b6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text = I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "text = book_text[:297]\n",
    "print(\"text = {}\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c51d78cf-d380-435b-aee0-ef5551335dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded = [53, 44, 149, 1003, 57, 38, 818, 115, 256, 486, 6, 1002, 115, 500, 435, 392, 6, 908, 585, 1077, 709, 508, 961, 1016, 663, 1016, 535, 987, 5, 568, 988, 538, 722, 549, 496, 5, 533, 514, 370, 549, 748, 5, 661, 115, 841, 1102, 5, 157, 397, 547, 568, 115, 1066, 727, 988, 84, 7, 3, 99, 53, 818, 1003, 585, 1120]\n",
      "decoded = 'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "encoded = tokenizer.encode(text)\n",
    "print(\"encoded = {}\".format(encoded))\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"decoded = '{}'\".format(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9821d959-18ce-416a-9c79-03aae61f9f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text == decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73be25-86de-4211-a070-438002b9a2d1",
   "metadata": {},
   "source": [
    "# Support non trained words and end of content tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d43eb3a-eaba-4727-afc7-2b011a6f662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_TOKEN = '<|unk|>'\n",
    "END_OF_TEXT_TOKEN = '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e18ed34-2363-47be-8e0a-291e6e6f0e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tks.extend([UNKNOWN_TOKEN, END_OF_TEXT_TOKEN])\n",
    "len(unique_tks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfca6973-d483-4d02-8f54-3c76062060b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {tk: idx for idx, tk in enumerate(unique_tks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1989650f-6186-4508-9b7c-31ac5e4753cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "        self.UNKNOWN_TOKEN = vocab[UNKNOWN_TOKEN]   # Change from V1\n",
    "\n",
    "    def encode(self, passage):\n",
    "        parts = re.split(r'([,.\\?\"_:;\\(\\)\\'!]|\\-\\-|\\s)', passage)\n",
    "        preprocessed = [tk for tk in parts if tk.strip()]\n",
    "        return [self.str_to_int.get(tk, self.UNKNOWN_TOKEN) for tk in preprocessed]   # Change from V1\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        processed = \" \".join(self.int_to_str[tk] for tk in tokens)\n",
    "        r1 = re.sub(r'\\s+([,.:;_\\)\"\\']|--)', r'\\1', processed)\n",
    "        # for ( & -- we should remove the suffix spaces\n",
    "        return re.sub(r'([\\(]|--)\\s+', r'\\1', r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3f4c25e-f6bb-4bfd-9f1e-59b114de7a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" {} \".format(END_OF_TEXT_TOKEN).join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "872671c9-061e-4cf1-9195-44bec88997aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1130, 1131]\n"
     ]
    }
   ],
   "source": [
    "print([vocab[UNKNOWN_TOKEN], vocab[END_OF_TEXT_TOKEN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c9b9df5-5db2-4ded-9835-60c17c71b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = SimpleTokenizerV2(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da0da37b-c9a7-4298-a7bf-7f859efc41da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded = [1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1130, 7]\n",
      "decoded = '<|unk|>, do you like tea ? <|endoftext|> In the sunlit terraces of the <|unk|>.'\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer2.encode(text)\n",
    "print(\"encoded = {}\".format(encoded))\n",
    "decoded = tokenizer2.decode(encoded)\n",
    "print(\"decoded = '{}'\".format(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cff65ba1-b494-4dd0-9823-4dd4c5448d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded == text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84487ab7-4d9a-46fd-be40-c101b4901cf5",
   "metadata": {},
   "source": [
    "# Use external Byte-pair tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cb5d74f-a46b-4ee8-9435-2004c69a262c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "tiktoken.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4bb3bfc-0bdd-415b-b673-560ec919e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98bddb3d-5d64-400d-a264-a47d303d581d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded = [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "decoded = 'Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.'\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    " \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    " \"of someunknownPlace.\"\n",
    ")\n",
    "encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "print(\"encoded = {}\".format(encoded))\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"decoded = '{}'\".format(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31ecd9ec-ea84-4083-967c-ef59a9125056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text == decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad8fa56e-1526-4b96-b7b0-9c40d1e9c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "['Ak', 'w', 'ir', 'w', ' ', 'ier']\n"
     ]
    }
   ],
   "source": [
    "example_encoding = tokenizer.encode(\"Akwirw ier\")  # This tokenizer works with anything\n",
    "print(example_encoding)\n",
    "print([tokenizer.decode([x]) for x in example_encoding])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e16a80-8a20-464d-8fce-ff1368843087",
   "metadata": {},
   "source": [
    "# Sliding window sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec2fd6e6-d7b7-4b57-9ede-c58a0437f690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5145"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_encoded = tokenizer.encode(book_text)\n",
    "len(book_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f89ab5f-8ac9-4afd-b4ab-b71a5f22ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_enc = book_encoded[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5dbdb77e-b6b5-4c5a-a079-d6d0bcfa3ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and  --->   established\n",
      " and established  --->   himself\n",
      " and established himself  --->   in\n",
      " and established himself in  --->   a\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "for i in range(context_size):\n",
    "    print(tokenizer.decode(sample_enc[:(i+1)]), \" ---> \", tokenizer.decode([sample_enc[i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c03c29a0-c34e-4854-bee9-3f9e1f28f9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0+rocm7.1'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7c42380-7974-4b5d-a3f9-8c26a11f1848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "087e04ec-3104-4411-89e7-b051e2e04525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b16be9d7-3ab4-49a4-bfae-3f9b18c507d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, window_length, stride):\n",
    "        tokens = tokenizer.encode(txt)\n",
    "        self.input_tokens = []\n",
    "        self.output_tokens = []\n",
    "        for i in range(0, len(tokens) - window_length, stride):\n",
    "            self.input_tokens.append(torch.tensor(tokens[i:(i+window_length)]))\n",
    "            self.output_tokens.append(torch.tensor(tokens[(i+1):(i+1+window_length)]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_tokens[idx], self.output_tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17e75cf4-294b-4eeb-b2d0-72ca86f5bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only function directly copied from the book\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    " tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    " dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    " dataloader = DataLoader(\n",
    "     dataset,\n",
    "     batch_size=batch_size,\n",
    "     shuffle=shuffle,\n",
    "     drop_last=drop_last,\n",
    "     num_workers=num_workers\n",
    "     )\n",
    " return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5a2bd2e-570c-4f38-b294-99170ee31dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(book_text, batch_size=2, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed501f0f-8595-40b6-95c5-c17b0da11207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:  [tensor([[  40,  367, 2885, 1464],\n",
      "        [ 367, 2885, 1464, 1807]]), tensor([[ 367, 2885, 1464, 1807],\n",
      "        [2885, 1464, 1807, 3619]])]\n",
      "Batch 2:  [tensor([[2885, 1464, 1807, 3619],\n",
      "        [1464, 1807, 3619,  402]]), tensor([[1464, 1807, 3619,  402],\n",
      "        [1807, 3619,  402,  271]])]\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch 1: \", next(data_iter))\n",
    "print(\"Batch 2: \", next(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a321e19-b81b-4c24-8f40-1f23d2299aec",
   "metadata": {},
   "source": [
    "# Token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1adfe730-4f99-45f5-b893-8ce597b2d51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "emb = torch.nn.Embedding(6,3)\n",
    "print(emb.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1da81a10-074d-45b9-b347-6a5886606e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [ 0.3374, -0.1778, -0.1690]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.tensor([2, 4, 0])\n",
    "embedding_vector = emb(token_ids)\n",
    "print(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1808e5a-dba2-40bc-ba0c-967fe309a249",
   "metadata": {},
   "source": [
    "# Encoding word positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3efea70-5852-4b52-93ac-34b494d9fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "emb = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "983e8ad6-ab50-4c16-8cde-29f9b7919897",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    " book_text, batch_size=8, max_length=max_length,\n",
    " stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32563413-b98b-44c7-9906-c5079b891bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "embeddings = emb(inputs)\n",
    "print(embeddings.shape) # each token is 256-d vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3016a47-7753-4179-8cc7-0985189217f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_len = max_length\n",
    "positional_embedding_layer = torch.nn.Embedding(ctx_len, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ddf973ad-1dc6-4ca4-a118-8816c2e357c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(ctx_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a98251f9-d0a9-4262-8f1a-4dd41c1da7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embs = positional_embedding_layer(torch.arange(ctx_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "256204ba-5694-4558-8ecd-54bb496e4d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6451, -0.6397,  0.3619,  ...,  0.4454,  0.4887,  0.5341],\n",
       "        [ 0.0851, -0.3358, -0.3749,  ...,  1.2503,  0.2494,  1.2698],\n",
       "        [ 1.4413,  0.1969,  0.0874,  ..., -0.6001, -0.0722,  0.4005],\n",
       "        [-1.5235,  0.8094,  0.4816,  ..., -0.2414, -1.5252,  1.0934]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b2ee41c0-63c0-4d9e-ab17-6ea243ceb1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e8754364-38a1-4653-8fa7-29ddef5f97d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(4, 256)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6fe58a3-fa86-4ced-a0e9-8430fc74bfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.6451, -0.6397,  0.3619,  ...,  0.4454,  0.4887,  0.5341],\n",
       "        [ 0.0851, -0.3358, -0.3749,  ...,  1.2503,  0.2494,  1.2698],\n",
       "        [ 1.4413,  0.1969,  0.0874,  ..., -0.6001, -0.0722,  0.4005],\n",
       "        [-1.5235,  0.8094,  0.4816,  ..., -0.2414, -1.5252,  1.0934]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a23cb58-d3e0-40b7-a5c0-0361e2c6189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = embeddings + position_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ddea2dc9-6a4d-49f4-a4b8-03a6a37a54c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211b173-582e-418c-9bf5-e06f2cb4dfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YALLM",
   "language": "python",
   "name": "yallm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
